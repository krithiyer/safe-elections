{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cs294s_2A_SimilarityCalculator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP8vh2iKYKznbl0BhnAki7u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "932f803db5194080a7c0ccfcebb7ce70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e9e29c9b37ba45779058e76ee2942f10",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ebf6acba7d234df983529b43645b6eeb",
              "IPY_MODEL_b67c5012d7be4dfdb554384079374078"
            ]
          }
        },
        "e9e29c9b37ba45779058e76ee2942f10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ebf6acba7d234df983529b43645b6eeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a8608756b7d34c329010634024be8d5f",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_08df4224d9d34439ad3dd92f9262a9d0"
          }
        },
        "b67c5012d7be4dfdb554384079374078": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_21837375aada4e188147ecbcf1b596fd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 821kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ce089f34320e49d4a340260fa33c5e96"
          }
        },
        "a8608756b7d34c329010634024be8d5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "08df4224d9d34439ad3dd92f9262a9d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "21837375aada4e188147ecbcf1b596fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ce089f34320e49d4a340260fa33c5e96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krithiyer/safe-elections/blob/main/cs294s_2A_SimilarityCalculator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Swm4QoMR7vV"
      },
      "source": [
        "###Extract word vector and calculate similarity\n",
        "\n",
        "Extract question embedding from different layers of BERT model finetuned on Quora Question Pairs dataset.\n",
        "\n",
        "Use different algorithmes to evaluate text similarity - match user question with questions in our (election / covid) curated FAQ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLjTtxI2TZKt"
      },
      "source": [
        "Connect to google drive and load the saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9Bu0oOtRx4u",
        "outputId": "44aa5e0a-7758-4d22-ee06-fb3a69bae626",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu-YAe_o1xM1",
        "outputId": "b3bc4514-da15-4581-aabd-70d71924fe32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/drive/My Drive/cs294s/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/cs294s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZILXSMDu2Rxz",
        "outputId": "f703aff6-08c9-4542-c5ae-cbd3edae1ce6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla V100-SXM2-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqNLFy3gThdy"
      },
      "source": [
        "install and import all needed packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWtwEw1iTjYQ",
        "outputId": "389cb5ad-c87b-470d-9c2f-e4d47b169c44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 8.4MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 33.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 55.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.0MB 50.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=63fcd01e012e28657b63b7280a5c0eeb71515d371451a5af621949e45b4cc94e\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdgiEb441Z6L",
        "outputId": "5fe45e5f-fae7-46b8-a440-a707601eebc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "932f803db5194080a7c0ccfcebb7ce70",
            "e9e29c9b37ba45779058e76ee2942f10",
            "ebf6acba7d234df983529b43645b6eeb",
            "b67c5012d7be4dfdb554384079374078",
            "a8608756b7d34c329010634024be8d5f",
            "08df4224d9d34439ad3dd92f9262a9d0",
            "21837375aada4e188147ecbcf1b596fd",
            "ce089f34320e49d4a340260fa33c5e96"
          ]
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "932f803db5194080a7c0ccfcebb7ce70",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXcx19imTknC"
      },
      "source": [
        "The model saved at './models_saved_qqp_oct9Final/' was fine tuned on QQP dataset for 4 epoch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soVJmnx1TnBN",
        "outputId": "0db8e9aa-f33b-4c05-ce32-5c0ac6c8f6ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "# The name of the folder containing the model files.\n",
        "input_dir = './models_saved_qqp_oct9Final/'\n",
        "\n",
        "# Load our fine-tuned model, and configure it to return the \"hidden states\", \n",
        "# from which we will be taking our text embeddings.\n",
        "# num_labels = 2,  is this needed while loading saved models\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    input_dir,\n",
        "    output_hidden_states = True, # Whether the model returns all hidden-states.\n",
        ") \n",
        "\n",
        "# Load the tokenizer. we have not saved it.\n",
        "#tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
        "\n",
        "# model.cuda()\n",
        "\n",
        "# Copy the model to the GPU.\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_S2-Qc6TnTC"
      },
      "source": [
        "Question to Embedding function. \n",
        "\n",
        "will take a question (from a user or from the curated FAQ list) and compute the word embedding vector for the question. The embedding vector of the user-question will be matched against the embeddings vectors of the questions from the FAQ collection to select responses.\n",
        "\n",
        "The embedding from the [CLS] token from the top layer is used. (Other experiments with averaging embeddings from different layers will be considered later)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I541X_FyUkf_"
      },
      "source": [
        "import torch\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def question_to_embedding(tokenizer, model, in_text):\n",
        "    '''\n",
        "    Uses the provided BERT `model` and `tokenizer` to generate a vector \n",
        "    representation of the input string, `in_text`.\n",
        "\n",
        "    Returns the vector stored as a numpy ndarray.\n",
        "    '''\n",
        "\n",
        "    MAX_LEN = 256\n",
        "\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Truncate the sentence to MAX_LEN if necessary.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end. (After truncating!)\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    input_ids = tokenizer.encode(\n",
        "                        in_text,                    # Sentence to encode.\n",
        "                        add_special_tokens = True,  # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = MAX_LEN,       # Truncate all sentences. \n",
        "                        truncation = True                       \n",
        "                   )    \n",
        "\n",
        "    # Pad our input tokens. Truncation was handled above by the `encode`\n",
        "    # function, which also makes sure that the `[SEP]` token is placed at the\n",
        "    # end *after* truncating.\n",
        "    # Note: `pad_sequences` expects a list of lists, but we only have one\n",
        "    # piece of text, so we surround `input_ids` with an extra set of brackets.\n",
        "    results = pad_sequences([input_ids], maxlen=MAX_LEN, dtype=\"long\", \n",
        "                              truncating=\"post\", padding=\"post\")\n",
        "    \n",
        "    # Remove the outer list.\n",
        "    input_ids = results[0]\n",
        "\n",
        "    # Create attention masks    \n",
        "    attn_mask = [int(i>0) for i in input_ids]\n",
        "    \n",
        "    # Cast to tensors.\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attn_mask = torch.tensor(attn_mask)\n",
        "\n",
        "    # Add an extra dimension for the \"batch\" (even though there is only one \n",
        "    # input in this batch.)\n",
        "    input_ids = input_ids.unsqueeze(0)\n",
        "    attn_mask = attn_mask.unsqueeze(0)\n",
        "\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Copy the inputs to the GPU\n",
        "\n",
        "    input_ids = input_ids.to(device)\n",
        "    attn_mask = attn_mask.to(device)\n",
        "    \n",
        "    # no need to build the backwards graph. \n",
        "    # this will make the model run a bit faster.\n",
        "\n",
        "    with torch.no_grad():        \n",
        "\n",
        "        # Forward pass, return hidden states and predictions.\n",
        "        # Note - since we are not supplying the lables to the model\n",
        "        # it will not return any loss.\n",
        "\n",
        "        logits, encoded_layers = model(\n",
        "                                    input_ids = input_ids, \n",
        "                                    token_type_ids = None, \n",
        "                                    attention_mask = attn_mask)\n",
        "        \n",
        "    # Retrieve our sentence embedding--\n",
        "    # experiment 1 : take the `[CLS]` embedding from the final\n",
        "    # layer.\n",
        "    # we can set the parameters below for different experimentation\n",
        "    # for embeddings.\n",
        "    \n",
        "    layer_i = 12 # The last BERT layer before the classifier.\n",
        "    batch_i = 0 # Only one input in the batch.\n",
        "    token_i = 0 # The first token, corresponding to [CLS]\n",
        "        \n",
        "    # Grab the embedding.\n",
        "    vec = encoded_layers[layer_i][batch_i][token_i]\n",
        "\n",
        "    # Move to the CPU and convert to numpy ndarray.\n",
        "    vec = vec.detach().cpu().numpy()\n",
        "\n",
        "    return(vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n2AE3VfVLLk"
      },
      "source": [
        "we also need to generate word embedding vectors for each question in the FAQ database.\n",
        "\n",
        "we need to read in the FAQ database - hopefully the FAQ database will be pre-processed in a notebook and readily aviable for reading\n",
        "\n",
        "To Do: read the FAQ database from a saved file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTq6JxbDTYOU",
        "outputId": "a299db16-5934-4c84-9b8f-9c801acaf0dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/cs294s'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nogTfMl8uejy"
      },
      "source": [
        "Initial curated QA consists of 1648 questions related to Covid Safety, Election Deadlines, etc.\n",
        "\n",
        "Questions scraped from the Secretary of State web sites (election data)\n",
        "and Covid safety data from different sites.\n",
        "\n",
        "Some Covid related questions taken from  https://openreview.net/pdf?id=qd51R0JNLl   (not all questions used - some dont have answers)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwDCttzvSxMe",
        "outputId": "1103eb31-e3da-47c7-82de-7786928abd1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import pandas\n",
        "election_faq_df = pandas.read_excel('./master_test1_v1_QA.xlsx')\n",
        "election_faq_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Due to COVID-19, can an election official requ...</td>\n",
              "      <td>No, an election official cannot require a vote...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Can election officials require voters to wear ...</td>\n",
              "      <td>No, you cannot require voters to wear face cov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>May election officials require an assistant/in...</td>\n",
              "      <td>No, just as you cannot require a voter to use ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>If an election worker cannot identify a voter ...</td>\n",
              "      <td>The election judge has discretion to ask the v...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How should election officials sanitize equipme...</td>\n",
              "      <td>The SOS recommends that election officials san...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Question                                             Answer\n",
              "0  Due to COVID-19, can an election official requ...  No, an election official cannot require a vote...\n",
              "1  Can election officials require voters to wear ...  No, you cannot require voters to wear face cov...\n",
              "2  May election officials require an assistant/in...  No, just as you cannot require a voter to use ...\n",
              "3  If an election worker cannot identify a voter ...  The election judge has discretion to ask the v...\n",
              "4  How should election officials sanitize equipme...  The SOS recommends that election officials san..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxUEb4ONvuUf"
      },
      "source": [
        "proc_data_df = election_faq_df.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XzHButawtdn",
        "outputId": "67e92a72-e81b-4925-a7f2-71af5d5be7d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "election_faq_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1647, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ0SsQxYTyFV"
      },
      "source": [
        "faq_q_embeddings = []\n",
        "for row in proc_data_df.itertuples():\n",
        "  vec = question_to_embedding(tokenizer, model, row.Question)\n",
        "  faq_q_embeddings.append(vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ik9yr1X4bcM",
        "outputId": "217257f7-ef20-4c29-c677-3c66c91bb249",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(faq_q_embeddings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1646"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmjQedsnWCs-",
        "outputId": "fc2fd9b1-c06b-4954-ead5-66755ff40587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert the list of vectors into a 2D array.\n",
        "vecs = np.stack(faq_q_embeddings)\n",
        "\n",
        "vecs.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1646, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APpn1lMcWZE2"
      },
      "source": [
        "np.save('./faq_q_oct12_embeddings.npy', vecs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryGuZ0vvVkh2"
      },
      "source": [
        "Need to read FAQ questions from the database - and iterate one question per call"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9n8aZ_QV0ob"
      },
      "source": [
        "Similar Question Search\n",
        "\n",
        "Cosine similarity is one option.\n",
        "\n",
        "Now that we have our comments all vectorized, we are ready to make them \"searchable\".\n",
        "\n",
        "We do this using a technique called \"k-Nearest Neighbor Search\" or \"k-NN\". Simply put, we use a distance metric such as Euclidean distance, calculate that distance between our \"query\" vector and all of the vectors to be searched, then sort the distances to find the closest matches.\n",
        "\n",
        "All of those distance calculations can make k-NN search computationally expensive and slow. There are a number of libraries out there for accelerating k-NN using carefully optimized code and / or approximation techniques.\n",
        "\n",
        "I personally like the FAISS (Facebook AI Similarity Search) library, in part because it has a really excellent GPU implementation.\n",
        "\n",
        "Using the GPU, we can perform \"brute-force\" k-NN search (meaning no approximation techniques which compromise on accuracy) on this dataset quickly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgQtTlR9HYiQ"
      },
      "source": [
        "more info on faiss\n",
        "\n",
        "https://github.com/facebookresearch/faiss/wiki/Getting-started\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OC7OruhWDe2",
        "outputId": "46d19d50-5bcb-47a3-d6c6-ba879b7f751d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!pip install faiss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting faiss\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/1c/4ae6cb87cf0c09c25561ea48db11e25713b25c580909902a92c090b377c0/faiss-1.5.3-cp36-cp36m-manylinux1_x86_64.whl (4.7MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.7MB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from faiss) (1.18.5)\n",
            "Installing collected packages: faiss\n",
            "Successfully installed faiss-1.5.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6htKiMbAWJGs",
        "outputId": "ab8d166e-12d8-4aac-d7b9-566393c4ba08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!pip install faiss-gpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting faiss-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/69/0e3f56024bb1423a518287673071ae512f9965d1faa6150deef5cc9e7996/faiss_gpu-1.6.3-cp36-cp36m-manylinux2010_x86_64.whl (35.5MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35.5MB 87kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from faiss-gpu) (1.18.5)\n",
            "Installing collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMVIxwGV7AbX"
      },
      "source": [
        "import faiss\n",
        "cpu_index = faiss.IndexFlatL2(vecs.shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7OmKMnBXQWX"
      },
      "source": [
        "cpu_index.add(vecs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-3AnV5lxoLe"
      },
      "source": [
        "Now that we have built the index for FAQ questions, let us experiment with a few questions\n",
        "\n",
        "following very trival example, using vector 8 to query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFnD4wQ4Xfhv"
      },
      "source": [
        "D, I = cpu_index.search(vecs[8].reshape(1, 768), k=5) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRnyrYaEXq9n",
        "outputId": "b442337d-be66-4db4-82ba-969033cf7a9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "for i in range(I.shape[1]):\n",
        "\n",
        "    # Look up the question row number for this result.\n",
        "    result_i = I[0, i]\n",
        "    print (result_i)\n",
        "    print(proc_data_df['Question'][result_i])\n",
        "    print(proc_data_df['Answer'][result_i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n",
            "May a poll watcher observe a voter being assisted in preparing their ballot?\n",
            "A watcher may not be present at the voting station when a voter is preparing the voterâ€™s ballot or is being assisted by a person of the voterâ€™s choice, including by a person also serving as an interpreter at the voting station.Â \n",
            "4\n",
            "How should election officials sanitize equipment after use?\n",
            "The SOS recommends that election officials sanitize equipment after each use, particularly if a voter is showing any signs or symptoms of COVID-19. Please contact your vendor about the specific procedures you should follow to clean and sanitize the equipment being used.\n",
            "7\n",
            "May election officials place social distancing requirements on a poll watcher?\n",
            "No, you cannot place social distancing requirements on a poll watcher. A person commits an offense if the person serves in an official capacity at a location at which the presence of watchers is authorized and knowingly prevents a watcher from observing an activity the watcher is entitled to observe.Â \n",
            "97\n",
            "When can I apply to vote absentee by mail?\n",
            "Apply now. Be sure that the election board receives your application before 11:59 PM on Thursday, October 22. Apply today in your voter portal by visitingÂ https://indianavoters.in.gov/.\n",
            "109\n",
            "Do I have to request an absentee ballot for each election?\n",
            "Yes, each request is only good for one election with limited exceptions. You will need to complete a new request for each election unless you qualify as a military or overseas voter or if you have been placed on Shelby Countyâ€™s Permanent Absentee Voting list.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH7-kB-6bWpX"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fg8IiH-wbX-t",
        "outputId": "7ddb1663-654d-4e64-d1bf-ac194d0c58d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "user_question = 'How can I avoid getting Covid  '\n",
        "\n",
        "user_query_vec = question_to_embedding(tokenizer, model, user_question)\n",
        "\n",
        "D, I = cpu_index.search(user_query_vec.reshape(1, 768), k=5) \n",
        "\n",
        "for i in range(I.shape[1]):\n",
        "\n",
        "    # Look up the comment row number for this result.\n",
        "    result_i = I[0, i]\n",
        "    print (result_i)\n",
        "    print(proc_data_df['Question'][result_i])\n",
        "    print(proc_data_df['Answer'][result_i])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "779\n",
            "what will be the educational impacts of covid\n",
            "read or watch local media sources that report school dismissals or and watch for communication from your childs school.\n",
            "865\n",
            "how do i stop covid\n",
            "the best way to prevent illness is to avoid being exposed to the virus. , stay at least 6 feet away from others, and wash your hands often. , regularly and thoroughly clean your hands with an alcohol based hand rub or wash them with soap and water. maintain at least 3 feet distance between yourself and anyone who is coughing or sneezing.\n",
            "748\n",
            "how can people help stop stigma related to covid\n",
            "people can fight stigma by providing social support in situations where you notice this is occurring.\n",
            "208\n",
            "can a person test negative and later test positive for covid\n",
            "a negative result means that the virus that causes covid was not found in the persons sample. in the early stages of infection, it is possible the virus will not be detected. \n",
            "835\n",
            "what should i do if i had close contact with someone who has covid\n",
            "stay home. call your doctor to talk about your symptoms and let them know youre coming for an appointment so they can prepare for your visit.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgtX9rKoysr2",
        "outputId": "692c1e5c-9e1f-44ed-aee8-791c2e2d82e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "user_question = 'Can i safely vote during covid  '\n",
        "\n",
        "user_query_vec = question_to_embedding(tokenizer, model, user_question)\n",
        "\n",
        "D, I = cpu_index.search(user_query_vec.reshape(1, 768), k=5) \n",
        "\n",
        "for i in range(I.shape[1]):\n",
        "\n",
        "    # Look up the comment row number for this result.\n",
        "    result_i = I[0, i]\n",
        "    print (result_i)\n",
        "    print(proc_data_df['Question'][result_i])\n",
        "    print(proc_data_df['Answer'][result_i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "902\n",
            "can the covid virus spread through drinking water\n",
            "there is no evidence that covid can be spread to humans through the use of pools and hot tubs.\n",
            "448\n",
            "how long is the incubation period for covid\n",
            "distribution is gamma distributed with mean 5.1 days and coefficient of variation, between 2 and 14 days, most estimates of the incubation period for covid range from 1 14 days, most commonly around five days.\n",
            "167\n",
            "are blood tests useful to diagnose covid\n",
            " there are many tests being used to diagnose covid that the u.s. food & drug administration (fda) has authorized for use during the current emergency. all of these diagnostic tests identify the virus in samples from the respiratory system, such as from nasal or nasopharyngeal swabs, all of these diagnostic tests identify the virus in samples from the respiratory system, such as from nasal or nasopharyngeal swabs. , rrt pcr test,  reverse transcription polymerase chain reaction, the 2109 ncov rna was readily detected in the blood (6 of 57 patients) and the anal swabs (11 of 28 patients). \n",
            "1411\n",
            "Is there a deadline to request my ballot in North Carolina\n",
            "In North Carolina one has to request a mail in ballot. \n",
            " The deadline to request is: 2020-10-27 For more information, please visit https://www.ncdot.gov/dmv/offices-services/online/Pages/voter-registration-application.aspx\n",
            "791\n",
            "can i test my pet for covid\n",
            "routine testing of pets for covid is not recommended at this time., no. at this time, routine testing of animals for covid is not recommended. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVYxUT3ey0Qo",
        "outputId": "c33f41f6-56c3-43cb-f72e-2366f21f7917",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "user_question = 'Can I request mailin ballot because of covid  '\n",
        "\n",
        "user_query_vec = question_to_embedding(tokenizer, model, user_question)\n",
        "\n",
        "D, I = cpu_index.search(user_query_vec.reshape(1, 768), k=5) \n",
        "\n",
        "for i in range(I.shape[1]):\n",
        "\n",
        "    # Look up the comment row number for this result.\n",
        "    result_i = I[0, i]\n",
        "    print (result_i)\n",
        "    print(proc_data_df['Question'][result_i])\n",
        "    print(proc_data_df['Answer'][result_i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "773\n",
            "what can i do to prepare my family for a covid outbreak\n",
            "practice everyday preventive actions to help reduce your risk of getting sick and remind everyone in your home to do the same., create a household plan of action to help protect your health and the health of those you care about in the event of an outbreak of covid in your community .\n",
            "442\n",
            "how can i protect my child from covid infection\n",
            "discourage children and teens from gathering in other public places while school is dismissed to help slow the spread of covid in the community. encourage frequent handwashing and follow other prevention tips., discourage children and teens from gathering in other public places while school is dismissed to help slow the spread of covid in the community., you can encourage your child to help stop the spread of covid by teaching them to do the same things everyone should do to stay healthy.\n",
            "1589\n",
            "When can I start voting in-person in Washington\n",
            "In Washington, in-person voting starts 2020-10-16\n",
            "788\n",
            "are covid tests being used on pets\n",
            "routine testing of pets for covid is not recommended at this time., no. at this time, routine testing of animals for covid is not recommended. \n",
            "1386\n",
            "Can I vote by mail in New Mexico\n",
            "In New Mexico one has to request a mail in ballot. \n",
            " The deadline to request is: 2020-10-20 For more information, please visit https://portal.sos.state.nm.us/OVR/(S(od4445h5uj2f5tyucvvhszdf))/WebPages/InstructionsStep1.aspx?AspxAutoDetectCookieSupport=1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}